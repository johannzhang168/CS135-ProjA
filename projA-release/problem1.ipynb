{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "data_dir = os.path.join('data_reviews/') \n",
    "x_train = pd.read_csv(data_dir+'x_train.csv')\n",
    "\n",
    "y_train = pd.read_csv(data_dir+'y_train.csv')['is_positive_sentiment']\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add custom stop words (example)\n",
    "custom_stop_words = [\"a\", \"an\", \"the\",\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\",\"and\", \"but\", \"or\", \"so\", \"yet\",\"in\", \"on\", \"at\", \"of\", \"to\", \"from\", \"with\",\"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\"because\", \"as\", \"until\", \"while\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "for word in custom_stop_words:\n",
    "    nlp.vocab[word].is_stop = True\n",
    "\n",
    "def processed_reviews(reviews):\n",
    "    processed = list()\n",
    "    for review in reviews:\n",
    "        review = review.lower()\n",
    "        review = re.sub(r\"[^a-zA-Z'\\s]\", '', review)\n",
    "        review = clean_negation(review)\n",
    "        doc = nlp(review)\n",
    "        lemmatized = [token.lemma_ for token in doc if not token.is_stop and token.pos_ in ['NOUN', 'ADJ']]\n",
    "        lemmatized = [word for word in lemmatized if word.strip()]\n",
    "        processed.append(' '.join(lemmatized))\n",
    "    return processed\n",
    "\n",
    "def clean_negation(text):\n",
    "    doc = nlp(text)\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'neg':\n",
    "            text = text.replace(token.text, '')\n",
    "            count += 1   \n",
    "    if count % 2 == 1:\n",
    "        text = text + ' negation'\n",
    "    return text\n",
    "\n",
    "def extract_BoW_features(texts):\n",
    "    processed_texts = [text[1] for text in texts]\n",
    "    processed_texts = processed_reviews(processed_texts)\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_df=0.5, min_df=1, binary=True)\n",
    "    features = vectorizer.fit_transform(processed_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    document_features = [\" \".join([feature_names[word_index] for word_index in doc_vector.indices]) for doc_vector in features]\n",
    "    return np.array(document_features)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression(max_iter=100000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'vectorizer__max_features': [1000, 2000, 3000, 4000],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vectorizer__binary': [True, False],\n",
    "    'vectorizer__min_df': [1, 2, 3, 4, 5],\n",
    "    'vectorizer__max_df': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'classifier__solver': ['liblinear', 'lbfgs', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter tuning\n",
    "random_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "random_search.fit(x_train['text'], y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "print(best_params)\n",
    "\n",
    "classifier1 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(\n",
    "        max_features=best_params['vectorizer__max_features'],\n",
    "        ngram_range=best_params['vectorizer__ngram_range'],\n",
    "        binary=best_params['vectorizer__binary'],\n",
    "        min_df=best_params['vectorizer__min_df'],\n",
    "        max_df=best_params['vectorizer__max_df']\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=best_params['classifier__C'],\n",
    "        solver=best_params['classifier__solver'],\n",
    "        max_iter=100000\n",
    "    ))\n",
    "])\n",
    "\n",
    "classifier1.fit(x_train['text'], y_train)\n",
    "\n",
    "def predict_proba(features):\n",
    "    return classifier1.predict_proba(features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train['text'], y_train, test_size=0.3, random_state=42)\n",
    "classifier1.fit(X_train, y_train)\n",
    "\n",
    "accuracy = classifier1.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "filename = 'best_model1.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(classifier1, file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
