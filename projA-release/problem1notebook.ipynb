{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "# import spacy\n",
    "# from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data_reviews/') \n",
    "x_train = pd.read_csv(data_dir+'x_train.csv')\n",
    "y_train = pd.read_csv(data_dir+'y_train.csv')['is_positive_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\johan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def fitCountVectorizer(texts):\n",
    "    processed_texts = [text[1].lower() for text in texts]\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    processed_texts = [text.translate(translator) for text in processed_texts]\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_texts = [[stemmer.stem(word) for word in text] for text in processed_texts]\n",
    "    processed_texts = [\"\".join(text) for text in processed_texts]\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7, min_df=1, binary=True) \n",
    "    vectorizer.fit(processed_texts)\n",
    "    return vectorizer\n",
    "\n",
    "texts = x_train.values.tolist()\n",
    "vectorizer = fitCountVectorizer(texts)\n",
    "filename = 'vectorizer.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "def extract_BoW_features(texts):\n",
    "    processed_texts = [text[1].lower() for text in texts]\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    processed_texts = [text.translate(translator) for text in processed_texts]\n",
    "    stemmer = PorterStemmer()\n",
    "    processed_texts = [[stemmer.stem(word) for word in text] for text in processed_texts]\n",
    "    processed_texts = [\"\".join(text) for text in processed_texts]\n",
    "    with open ('./vectorizer.pkl', 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    features = vectorizer.transform(processed_texts)\n",
    "    return features.toarray()\n",
    "# import torch\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "# from sklearn.preprocessing import MaxAbsScaler\n",
    "# def extract_BoW_features(texts):\n",
    "#     processed_texts = [text[1] for text in texts]  \n",
    "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#     model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#     inputs = tokenizer(processed_texts, padding=True, truncation=True, return_tensors='pt', max_length=1024)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#     embeddings = outputs.last_hidden_state\n",
    "#     features = torch.mean(embeddings, dim=1)\n",
    "#     features_numpy = features.numpy()\n",
    "#     scaler = MaxAbsScaler()\n",
    "#     features_numpy = scaler.fit_transform(features_numpy)\n",
    "#     return features_numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = extract_BoW_features(x_train.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 4424)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dense_x_train_features \u001b[38;5;241m=\u001b[39m \u001b[43mx_train_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m()\n\u001b[0;32m      2\u001b[0m dense_x_train_features\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "dense_x_train_features = x_train_features.toarray()\n",
    "dense_x_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'solver': ['liblinear', 'lbfgs', 'newton-cholesky'],\n",
    "    'penalty': ['l2', 'l1'],\n",
    "    'C' : np.logspace(-3, 2, 20),\n",
    "    'tol': [1e-10, 1e-8, 1e-6, 1e-5, 1e-4, 1e-3],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_splits = 10\n",
    "stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "310 fits failed out of a total of 1000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "140 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "170 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.64694444        nan        nan        nan 0.87225347\n",
      " 0.85693403 0.88388542 0.87225347 0.86252431        nan 0.85530903\n",
      " 0.85406597        nan 0.85385069 0.87717014        nan 0.85693403\n",
      " 0.85434375 0.85467014 0.60592708 0.5        0.86258681 0.86326389\n",
      " 0.85467014        nan        nan        nan 0.87705903        nan\n",
      "        nan 0.85909375        nan        nan 0.87221875 0.87703819\n",
      " 0.88388542 0.5        0.86214931 0.87225347 0.85387847        nan\n",
      " 0.85423958 0.86697569 0.88112153 0.88089236        nan 0.87797569\n",
      "        nan        nan 0.83448264        nan 0.85415278 0.87629514\n",
      "        nan        nan 0.78389583        nan 0.88089236 0.87719097\n",
      " 0.86478125        nan 0.87144097 0.88107292 0.88361458 0.86492708\n",
      " 0.85467014 0.60592708        nan 0.83580208 0.85713542 0.86478125\n",
      " 0.78389583 0.5        0.88364236        nan 0.87143403 0.86700347\n",
      " 0.85434375        nan 0.87220486 0.88087153 0.85690625 0.85385069\n",
      "        nan 0.85467014 0.85712847        nan 0.87225347 0.86253819\n",
      " 0.5        0.84373958        nan 0.70281944 0.5        0.86700347\n",
      "        nan 0.86700347        nan 0.87705903]\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1216: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 16.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "model1 = LogisticRegression(max_iter=7000)\n",
    "\n",
    "random_search = RandomizedSearchCV(model1,\n",
    "                                   param_distributions=param_grid,\n",
    "                                   cv=stratified_kfold,\n",
    "                                   n_iter=100,\n",
    "                                   scoring='roc_auc')\n",
    "with parallel_backend('threading'):\n",
    "    random_search.fit(x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Best Hyperparameters: {'tol': 0.001, 'solver': 'liblinear', 'penalty': 'l2', 'C': 4.832930238571752}\n",
      " \n",
      "Best Score: 0.8838854166666665\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(' ')\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 4424)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2573)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = Pipeline([\n",
    "        ('classifer', LogisticRegression(**random_search.best_params_)\n",
    ")\n",
    "])\n",
    "\n",
    "best_model.fit(x_train_features, y_train)\n",
    "\n",
    "# y_hat = best_model.predict(te_text_features)\n",
    "\n",
    "# y_hat\n",
    "\n",
    "filename = 'best_model1.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# accuracy = best_model.score(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('cs135_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb8adf67dd6994069ce8165d3f48ac504302bfdd73047ad73b834acd8b28d7c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
